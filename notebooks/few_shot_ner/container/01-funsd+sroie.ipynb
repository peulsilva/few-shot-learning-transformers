{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from torch.utils.data import DataLoader\n",
    "from torcheval.metrics.functional import multiclass_f1_score, multiclass_confusion_matrix\n",
    "from copy import deepcopy\n",
    "from IPython.display import clear_output\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "import os \n",
    "while 'notebooks' in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "    \n",
    "from src.preprocessing.make_dataset import ImageLayoutDataset\n",
    "from src.model.trainer import BertTrainer, LayoutLMTrainer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21cd65556bf747b8a11d742f32d6f748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘logs’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "model_name = \"few_shot_learning\"\n",
    "dataset_name = \"sroie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='logs/few_shot_learning_cord.log', encoding='utf-8', level= logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing source and support dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset = load_dataset(\n",
    "    \"darentang/sroie\",\n",
    "    cache_dir = \"/Data/pedro.silva/\"\n",
    ")\n",
    "\n",
    "support_dataset = load_dataset(\n",
    "    \"nielsr/funsd\",\n",
    "    cache_dir = \"/Data/pedro.silva/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMModel were not initialized from the model checkpoint at microsoft/layoutlm-base-uncased and are newly initialized: ['layoutlm.embeddings.word_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\n",
    "    \"microsoft/layoutlm-base-uncased\",\n",
    "    cache_dir = \"/Data/pedro.silva/\"\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/layoutlm-base-uncased\",\n",
    "    cache_dir = \"/Data/pedro.silva/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-COMPANY',\n",
       " 'I-COMPANY',\n",
       " 'B-DATE',\n",
       " 'I-DATE',\n",
       " 'B-ADDRESS',\n",
       " 'I-ADDRESS',\n",
       " 'B-TOTAL',\n",
       " 'I-TOTAL']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_dataset['train'].features['ner_tags'].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/626 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 626/626 [00:03<00:00, 188.64it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_labels ={\n",
    "    0: 0,\n",
    "    1: 1,\n",
    "    2: 1,\n",
    "    3: 2,\n",
    "    4: 2,\n",
    "    5: 3,\n",
    "    6: 3,\n",
    "    7: 4,\n",
    "    8: 4\n",
    "}\n",
    "\n",
    "source_df = ImageLayoutDataset(\n",
    "    source_dataset['train'],\n",
    "    tokenizer,\n",
    "    valid_labels_keymap= valid_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = source_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(\n",
    "    input_ids=input['input_ids'].reshape(1,-1), \n",
    "    bbox= input['bbox'].reshape([1, 512, 4]),\n",
    "    attention_mask=input['attention_mask'].reshape(1,-1), \n",
    "    token_type_ids=input['token_type_ids'].reshape(1,-1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F_mean(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim:int, \n",
    "                 output_dim: int, \n",
    "                 device : str = 'cuda',\n",
    "                 *args, \n",
    "                 **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.layer1 = torch.nn.Linear(input_dim, 512).to(device)\n",
    "        self.layer2 = torch.nn.Linear(512, output_dim).to(device)\n",
    "        self.elu = torch.nn.ELU().to(device)\n",
    "        self.relu = torch.nn.ReLU().to(device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return self.elu(x).squeeze()\n",
    "\n",
    "class F_cov(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim:int, \n",
    "                 output_dim: int, \n",
    "                 device : str = 'cuda',\n",
    "                 epsilon : float = 1e-14,\n",
    "                 *args, \n",
    "                 **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.layer1 = torch.nn.Linear(input_dim, 512).to(device)\n",
    "        self.layer2 = torch.nn.Linear(512, output_dim).to(device)\n",
    "        self.elu = torch.nn.ELU().to(device)\n",
    "        self.relu = torch.nn.ReLU().to(device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.elu(x)\n",
    "        x += 1 + self.epsilon\n",
    "\n",
    "        # return torch.diag(x)\n",
    "        return torch.diag(x.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 768\n",
    "output_dim = 64\n",
    "\n",
    "f_mean = F_mean(\n",
    "    input_dim,\n",
    "    output_dim\n",
    ")\n",
    "\n",
    "f_cov = F_cov(\n",
    "    input_dim,\n",
    "    output_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.last_hidden_state[:,0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(f_mean.parameters()) + list(f_cov.parameters()) + list(model.parameters())\n",
    "optimizer = torch.optim.Adam(\n",
    "    params,\n",
    "    lr = 1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on source set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_div(\n",
    "    mu_1 : torch.Tensor,\n",
    "    mu_2 : torch.Tensor,\n",
    "    cov_1: torch.Tensor,\n",
    "    cov_2: torch.Tensor\n",
    "):\n",
    "    l = cov_1.shape[0]\n",
    "    inv_1 = cov_1.inverse()\n",
    "    # inv_2 = cov_2.inverse()\n",
    "    kl = 1/2 * (mu_1 - mu_2).T @ inv_1 @ (mu_1 - mu_2)\n",
    "    kl+= 1/2 * torch.trace(inv_1 @ cov_2)\n",
    "    kl+= -l/2\n",
    "    kl+= torch.log(\n",
    "        torch.det(cov_1)/torch.det(cov_2)\n",
    "    )\n",
    "\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-100], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_df[0]['labels'][:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = len(source_df)\n",
    "minibatch_size = 5\n",
    "total_batches = total_samples // minibatch_size\n",
    "\n",
    "# Initialize a list to store minibatches\n",
    "minibatches = []\n",
    "\n",
    "# Loop over the total number of batches\n",
    "for _ in range(total_batches):\n",
    "    # Generate random indices for the minibatch\n",
    "    random_indices = np.random.choice(total_samples, minibatch_size, replace=False)\n",
    "    # Append the minibatch to the list\n",
    "    minibatches.append(random_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(minibatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_f_mean = \"/Data/pedro.silva/f_mean_sroie.pt\"\n",
    "checkpoint_f_cov = \"/Data/pedro.silva/f_cov_sroie.pt\"\n",
    "checkpoint_llm = \"peulsilva/container-source-sroie-checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 9\n",
      "loss: 0.001353245577774942\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7054af796e4e4b8974a2dc906e1c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/451M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch_idx in range(len(minibatches[0:10])):\n",
    "    loss = 0\n",
    "    total_points = 0\n",
    "    for document_idx in tqdm(minibatches[batch_idx]):\n",
    "        means = {}\n",
    "        covs = {}\n",
    "        X_p = 0\n",
    "        s_X = 0\n",
    "        n_p = 0\n",
    "\n",
    "        size = source_df[document_idx]['labels'].shape[1]\n",
    "        input = source_df[document_idx]\n",
    "\n",
    "        out = model(\n",
    "            input_ids=input['input_ids'].reshape(1,-1), \n",
    "            bbox= input['bbox'].reshape([1, 512, 4]),\n",
    "            attention_mask=input['attention_mask'].reshape(1,-1), \n",
    "            token_type_ids=input['token_type_ids'].reshape(1,-1),\n",
    "        )\n",
    "\n",
    "        for i in range(size):\n",
    "            if source_df[document_idx]['labels'][:,i] == -100:\n",
    "                continue\n",
    "            mu_i = f_mean(out.last_hidden_state[:,i,:])\n",
    "            cov_i = f_cov(out.last_hidden_state[:,i, :])\n",
    "\n",
    "            means[i] = mu_i\n",
    "            covs[i] = cov_i\n",
    "\n",
    "        del out\n",
    "\n",
    "        for i in range(size):\n",
    "            if source_df[document_idx]['labels'][:,i] == -100:\n",
    "                continue\n",
    "\n",
    "\n",
    "            total_points+= 1\n",
    "            for j in range(i+1, size):\n",
    "                if source_df[document_idx]['labels'][:,j] == -100:\n",
    "                    continue\n",
    "\n",
    "                # mu_i = f_mean(out.last_hidden_state[:,i,:])\n",
    "                # cov_i = f_cov(out.last_hidden_state[:,i, :])\n",
    "                # mu_j = f_mean(out.last_hidden_state[:,j,:])\n",
    "                # cov_j = f_cov(out.last_hidden_state[:,j, :])\n",
    "                mu_i = means[i]\n",
    "                cov_i = covs[i]\n",
    "                mu_j = means[j]\n",
    "                cov_j = covs[j]\n",
    "                d_ij = 1/2 * (KL_div(mu_i, mu_j, cov_i, cov_j) + KL_div(mu_j, mu_i, cov_j, cov_i))\n",
    "\n",
    "                if source_df[document_idx]['labels'][:,i] == source_df[document_idx]['labels'][:,j]:\n",
    "                    X_p += torch.exp(-d_ij)\n",
    "                    n_p += 1\n",
    "\n",
    "                s_X += torch.exp(-d_ij)\n",
    "\n",
    "        del means\n",
    "        del covs\n",
    "\n",
    "        loss -= torch.log(X_p / n_p / s_X)\n",
    "        \n",
    "\n",
    "    loss = loss/total_points\n",
    "    clear_output()\n",
    "    print(f\"batch {batch_idx}\")\n",
    "    print(f\"loss: {loss.item()}\")\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "model.push_to_hub(checkpoint_llm)\n",
    "torch.save(f_mean.state_dict(), checkpoint_f_mean)\n",
    "torch.save(f_cov.state_dict(), checkpoint_f_cov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0014, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few shot learning on support domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-HEADER',\n",
       " 'I-HEADER',\n",
       " 'B-QUESTION',\n",
       " 'I-QUESTION',\n",
       " 'B-ANSWER',\n",
       " 'I-ANSWER']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support_dataset['train'].features['ner_tags'].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/149 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [00:01<00:00, 112.24it/s]\n"
     ]
    }
   ],
   "source": [
    "n_shots = 2\n",
    "valid_labels_funsd = {\n",
    "    0: 0,\n",
    "    1:1,\n",
    "    2:1,\n",
    "    3:2,\n",
    "    4:2,\n",
    "    5:3,\n",
    "    6:3\n",
    "}\n",
    "support_df = ImageLayoutDataset(\n",
    "    support_dataset['train'],\n",
    "    tokenizer,\n",
    "    valid_labels_keymap=valid_labels_funsd\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3bcd5dff499452f8db2cb8aa75c2d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/451M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_dim = 768\n",
    "output_dim = 64\n",
    "\n",
    "f_mean = F_mean(\n",
    "    input_dim,\n",
    "    output_dim\n",
    ")\n",
    "\n",
    "f_cov = F_cov(\n",
    "    input_dim,\n",
    "    output_dim\n",
    ")\n",
    "\n",
    "f_mean.load_state_dict(torch.load(checkpoint_f_mean))\n",
    "f_cov.load_state_dict(torch.load(checkpoint_f_cov))\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    checkpoint_llm,\n",
    "    cache_dir = '/Data/pedro.silva/'\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(f_mean.parameters()) + list(f_cov.parameters()) + list(model.parameters())\n",
    "optimizer_support = torch.optim.Adam(\n",
    "    params,\n",
    "    lr = 1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_prev = torch.inf\n",
    "loss_ft = 1e100\n",
    "loss_ft < loss_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2\n",
      "loss: 0.0005063646822236478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:17<00:17, 17.20s/it]"
     ]
    }
   ],
   "source": [
    "loss_prev = torch.inf\n",
    "loss_ft = 1e100\n",
    "epoch = 0\n",
    "\n",
    "while loss_ft < loss_prev:\n",
    "    epoch+=1\n",
    "\n",
    "    loss_prev = loss_ft\n",
    "    loss_ft = 0\n",
    "    total_points = 0\n",
    "    for document_idx in tqdm(range(len(support_df[0:n_shots]))):\n",
    "        means = {}\n",
    "        covs = {}\n",
    "        X_p = 0\n",
    "        s_X = 0\n",
    "        n_p = 0\n",
    "\n",
    "        size = support_df[document_idx]['labels'].shape[1]\n",
    "        input = support_df[document_idx]\n",
    "\n",
    "        out = model(\n",
    "            input_ids=input['input_ids'].reshape(1,-1), \n",
    "            bbox= input['bbox'].reshape([1, 512, 4]),\n",
    "            attention_mask=input['attention_mask'].reshape(1,-1), \n",
    "            token_type_ids=input['token_type_ids'].reshape(1,-1),\n",
    "        )\n",
    "\n",
    "        for i in range(size):\n",
    "            if support_df[document_idx]['labels'][:,i] == -100:\n",
    "                continue\n",
    "            mu_i = f_mean(out.last_hidden_state[:,i,:])\n",
    "            cov_i = f_cov(out.last_hidden_state[:,i, :])\n",
    "\n",
    "            means[i] = mu_i\n",
    "            covs[i] = cov_i\n",
    "\n",
    "        del out\n",
    "\n",
    "        for i in range(size):\n",
    "            if support_df[document_idx]['labels'][:,i] == -100:\n",
    "                continue\n",
    "\n",
    "            total_points+= 1\n",
    "            for j in range(i+1, size):\n",
    "                if support_df[document_idx]['labels'][:,j] == -100:\n",
    "                    continue\n",
    "\n",
    "                # mu_i = f_mean(out.last_hidden_state[:,i,:])\n",
    "                # cov_i = f_cov(out.last_hidden_state[:,i, :])\n",
    "                # mu_j = f_mean(out.last_hidden_state[:,j,:])\n",
    "                # cov_j = f_cov(out.last_hidden_state[:,j, :])\n",
    "                mu_i = means[i]\n",
    "                cov_i = covs[i]\n",
    "                mu_j = means[j]\n",
    "                cov_j = covs[j]\n",
    "                d_ij = 1/2 * (KL_div(mu_i, mu_j, cov_i, cov_j) + KL_div(mu_j, mu_i, cov_j, cov_i))\n",
    "\n",
    "                if support_df[document_idx]['labels'][:,i] == support_df[document_idx]['labels'][:,j]:\n",
    "                    X_p += torch.exp(-d_ij)\n",
    "                    n_p += 1\n",
    "\n",
    "                s_X += torch.exp(-d_ij)\n",
    "\n",
    "        del means\n",
    "        del covs\n",
    "\n",
    "        loss_ft -= torch.log(X_p / n_p / s_X)\n",
    "        \n",
    "\n",
    "    loss_ft = loss_ft/total_points\n",
    "    clear_output()\n",
    "    print(f\"epoch {epoch}\")\n",
    "    print(f\"loss: {loss_ft.item()}\")\n",
    "\n",
    "    loss_ft.backward()\n",
    "\n",
    "    optimizer_support.step()\n",
    "\n",
    "    optimizer_support.zero_grad()\n",
    "\n",
    "model.push_to_hub(f\"peulsilva/container-source-sroie-funsd\")\n",
    "torch.save(f_mean.state_dict(), \"/Data/pedro.silva/f_mean_sroie-2.pt\")\n",
    "torch.save(f_cov.state_dict(), \"/Data/pedro.silva/f_cov_sroie-2.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for document_idx in tqdm(range(len(support_df[0:n_shots]))):\n",
    "        X_p = 0\n",
    "        s_X = 0\n",
    "        n_p = 0\n",
    "\n",
    "        size = support_df[document_idx]['labels'].shape[1]\n",
    "\n",
    "        out = model(\n",
    "            input_ids=input['input_ids'].reshape(1,-1), \n",
    "            bbox= input['bbox'].reshape([1, 512, 4]),\n",
    "            attention_mask=input['attention_mask'].reshape(1,-1), \n",
    "            token_type_ids=input['token_type_ids'].reshape(1,-1),\n",
    "        )\n",
    "\n",
    "        for i in range(size):\n",
    "            if support_df[document_idx]['labels'][:,i] == -100:\n",
    "                continue\n",
    "\n",
    "            h_i = out.last_hidden_state[:,i,:]\n",
    "\n",
    "\n",
    "for document_idx in tqdm((range(len(support_df[n_shots:])))):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
