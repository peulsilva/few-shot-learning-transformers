{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import LayoutLMv2Processor, LayoutLMForTokenClassification, AdamW, BertForTokenClassification,\\\n",
    "    BertTokenizer, LayoutLMTokenizer, AutoTokenizer, AutoModelForMaskedLM, pipeline, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.nn.functional import cross_entropy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torcheval.metrics.functional import multiclass_f1_score, multiclass_confusion_matrix\n",
    "from copy import deepcopy, copy\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "import os \n",
    "while 'notebooks' in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "import re\n",
    "from typing import List, Dict\n",
    "    \n",
    "from src.preprocessing.make_dataset import ImageLayoutDataset, PatternExploitingDataset, SplitWordsDataset\n",
    "from src.model.mlm.trainer import MLMTrainer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative PET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘logs’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir logs\n",
    "model_name = \"iPET\"\n",
    "dataset_name = \"FUNSD\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logging.basicConfig(filename=f'logs/{model_name}_{dataset_name}.log', encoding='utf-8', level= logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging in to huggingface hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.9/site-packages (8.1.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.9/site-packages (from ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.9/site-packages (from ipywidgets) (8.15.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.9/site-packages (from ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in ./.venv/lib/python3.9/site-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in ./.venv/lib/python3.9/site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: backcall in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.0)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.16.1)\n",
      "Requirement already satisfied: stack-data in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.7.1)\n",
      "Requirement already satisfied: exceptiongroup in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.3)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.venv/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.9/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.9/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.0)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.9/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611aa0a896d34ec0951ef5b9d8591375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing models and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"nielsr/funsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern0(\n",
    "    phrase : str,\n",
    "    word : str,\n",
    "    tokenizer\n",
    "):\n",
    "    return f\"In the phrase '{phrase}', the {word} is in the header, in a question, or in an answer? {tokenizer.mask_token}\"\n",
    "\n",
    "def pattern1(\n",
    "    phrase : str, \n",
    "    word : str,\n",
    "    tokenizer\n",
    "):\n",
    "    return f\"In the phrase '{phrase}', where is the {word} is situated at? {tokenizer.mask_token}\"\n",
    "\n",
    "def pattern3(\n",
    "    phrase : str,\n",
    "    word : str, \n",
    "    tokenizer\n",
    "):\n",
    "    return f\"Question: In the phrase '{phrase}', \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_list = [pattern0, pattern1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [00:00<00:00, 2475.08it/s]\n",
      "100%|██████████| 149/149 [00:00<00:00, 2524.32it/s]\n"
     ]
    }
   ],
   "source": [
    "train_datas = [\n",
    "    SplitWordsDataset(\n",
    "        dataset['train'],\n",
    "        tokenizer,\n",
    "        pattern\n",
    "    )\n",
    "\n",
    "    for pattern in pattern_list\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 1874.63it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 2133.44it/s]\n"
     ]
    }
   ],
   "source": [
    "test_datas =  [\n",
    "    SplitWordsDataset(\n",
    "        dataset['test'],\n",
    "        tokenizer,\n",
    "        pattern\n",
    "    )\n",
    "\n",
    "    for pattern in pattern_list\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbalizer = {\n",
    "    \"none\": 0,\n",
    "    \"question\" : 1,\n",
    "    \"answer\": 2,\n",
    "    \"header\" : 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train all models with patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "models = [\n",
    "    AutoModelForMaskedLM.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
    "    for i in range(len(pattern_list))\n",
    "]\n",
    "\n",
    "trainers = [\n",
    "    MLMTrainer(models[i], tokenizer, verbalizer )\n",
    "    for i in range(len(pattern_list))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:26<00:00,  8.64s/it]\n",
      "100%|██████████| 10/10 [00:56<00:00,  5.69s/it]\n",
      "100%|██████████| 10/10 [01:25<00:00,  8.54s/it]\n",
      "100%|██████████| 10/10 [00:56<00:00,  5.62s/it]\n",
      "100%|██████████| 10/10 [01:24<00:00,  8.44s/it]\n",
      "100%|██████████| 10/10 [00:54<00:00,  5.45s/it]\n",
      "100%|██████████| 10/10 [01:23<00:00,  8.32s/it]\n",
      "100%|██████████| 10/10 [00:54<00:00,  5.42s/it]\n",
      "100%|██████████| 10/10 [01:22<00:00,  8.24s/it]\n",
      "100%|██████████| 10/10 [00:54<00:00,  5.40s/it]\n",
      "100%|██████████| 10/10 [01:22<00:00,  8.30s/it]\n",
      "100%|██████████| 10/10 [00:53<00:00,  5.38s/it]\n",
      "100%|██████████| 10/10 [01:22<00:00,  8.22s/it]\n",
      "100%|██████████| 10/10 [00:54<00:00,  5.42s/it]\n",
      "100%|██████████| 10/10 [01:22<00:00,  8.26s/it]\n",
      "100%|██████████| 10/10 [00:53<00:00,  5.39s/it]\n",
      "100%|██████████| 10/10 [01:21<00:00,  8.20s/it]\n",
      "100%|██████████| 10/10 [00:53<00:00,  5.38s/it]\n",
      "100%|██████████| 10/10 [01:22<00:00,  8.23s/it]\n",
      "100%|██████████| 10/10 [00:53<00:00,  5.38s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd45465684f496fb687d8207e4f7460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:22<00:00,  8.24s/it]\n",
      "100%|██████████| 10/10 [00:54<00:00,  5.47s/it]\n",
      "100%|██████████| 10/10 [01:22<00:00,  8.25s/it]\n",
      "100%|██████████| 10/10 [00:53<00:00,  5.38s/it]\n",
      "100%|██████████| 10/10 [01:23<00:00,  8.33s/it]\n",
      "100%|██████████| 10/10 [00:53<00:00,  5.36s/it]\n",
      "100%|██████████| 10/10 [01:21<00:00,  8.16s/it]\n",
      "100%|██████████| 10/10 [00:54<00:00,  5.48s/it]\n",
      "100%|██████████| 10/10 [01:21<00:00,  8.17s/it]\n",
      "100%|██████████| 10/10 [00:54<00:00,  5.44s/it]\n",
      "100%|██████████| 10/10 [01:22<00:00,  8.24s/it]\n",
      "100%|██████████| 10/10 [00:54<00:00,  5.46s/it]\n",
      "100%|██████████| 10/10 [01:21<00:00,  8.14s/it]\n",
      "100%|██████████| 10/10 [00:54<00:00,  5.43s/it]\n",
      "100%|██████████| 10/10 [01:21<00:00,  8.14s/it]\n",
      "100%|██████████| 10/10 [00:54<00:00,  5.43s/it]\n",
      "100%|██████████| 10/10 [01:21<00:00,  8.17s/it]\n",
      "100%|██████████| 10/10 [00:53<00:00,  5.38s/it]\n",
      "100%|██████████| 10/10 [01:22<00:00,  8.23s/it]\n",
      "100%|██████████| 10/10 [00:53<00:00,  5.37s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2613cbdeaa1d426ba57cef989bad845d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx, trainer in enumerate(trainers):\n",
    "    logging.info(f\"Training the {idx}th model\")\n",
    "    train_data = train_datas[idx]\n",
    "\n",
    "    trainer.compile(\n",
    "        train_data,\n",
    "        n_shots=10,\n",
    "\n",
    "    )\n",
    "    trainer.best_model.push_to_hub(f\"peulsilva/ipet-pattern{idx}-10-shots\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing iPET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [[\n",
    "    AutoModelForMaskedLM.from_pretrained(f\"peulsilva/ipet-pattern{i}-10-shots\").to(device)\n",
    "    for i in range(2)\n",
    "]]\n",
    "\n",
    "trainers = [\n",
    "    MLMTrainer(models[0][i], tokenizer, verbalizer )\n",
    "    for i in range(len(pattern_list))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:44<00:00,  4.45s/it]\n",
      "100%|██████████| 10/10 [00:43<00:00,  4.34s/it]\n",
      "100%|██████████| 10/10 [01:22<00:00,  8.26s/it]\n",
      "100%|██████████| 10/10 [00:39<00:00,  3.95s/it]\n",
      "100%|██████████| 10/10 [01:23<00:00,  8.38s/it]\n",
      "100%|██████████| 10/10 [00:40<00:00,  4.05s/it]\n",
      "100%|██████████| 10/10 [01:25<00:00,  8.56s/it]\n",
      "100%|██████████| 10/10 [00:40<00:00,  4.05s/it]\n",
      "100%|██████████| 10/10 [01:25<00:00,  8.57s/it]\n",
      "100%|██████████| 10/10 [00:40<00:00,  4.02s/it]\n",
      " 60%|██████    | 6/10 [00:56<00:37,  9.36s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/pet/02-ipet.ipynb Cell 20\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.98/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/pet/02-ipet.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_patterns):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.98/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/pet/02-ipet.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     trainer \u001b[39m=\u001b[39m MLMTrainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.98/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/pet/02-ipet.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m         models[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][i],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.98/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/pet/02-ipet.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m         tokenizer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.98/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/pet/02-ipet.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m         verbalizer\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.98/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/pet/02-ipet.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.98/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/pet/02-ipet.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     trainer\u001b[39m.\u001b[39;49mcompile(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.98/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/pet/02-ipet.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m         T_ij[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m][i],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.98/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/pet/02-ipet.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m         \u001b[39m10\u001b[39;49m\u001b[39m*\u001b[39;49m(j\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.98/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/pet/02-ipet.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.98/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/pet/02-ipet.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     trainer\u001b[39m.\u001b[39mbest_model\u001b[39m.\u001b[39mpush_to_hub(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpeulsilva/ipet-model\u001b[39m\u001b[39m{\u001b[39;00mj\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m-pattern\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m-10-shots\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/src/model/mlm/trainer.py:87\u001b[0m, in \u001b[0;36mMLMTrainer.compile\u001b[0;34m(self, train_data, n_shots, n_epochs, n_validation)\u001b[0m\n\u001b[1;32m     84\u001b[0m mask_token_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(tokens[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mmask_token_id)[\u001b[39m1\u001b[39m]\n\u001b[1;32m     85\u001b[0m mask_token_logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits[\u001b[39m0\u001b[39m, mask_token_index, :]\n\u001b[0;32m---> 87\u001b[0m question_logits \u001b[39m=\u001b[39m mask_token_logits[\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mvocab[\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m]]\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     88\u001b[0m answer_logits \u001b[39m=\u001b[39m mask_token_logits[\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mvocab[\u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m]]\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     89\u001b[0m header_logits \u001b[39m=\u001b[39m mask_token_logits[\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mvocab[\u001b[39m\"\u001b[39m\u001b[39mheader\u001b[39m\u001b[39m\"\u001b[39m]]\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/transformers/tokenization_utils_fast.py:181\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvocab\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]:\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_vocab()\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/transformers/tokenization_utils_fast.py:177\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.get_vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_vocab\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]:\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mget_vocab(with_added_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_models = 5\n",
    "n_patterns = 2\n",
    "T_ij = [deepcopy(train_datas)]\n",
    "for j in range(n_models):\n",
    "    logging.info(f\"Evaluating dataset with {(j+1)*5} shots\")\n",
    "\n",
    "    # evaluate\n",
    "    new_T = []\n",
    "    for i in range(n_patterns):\n",
    "        Lambda = np.random.randint(1,2)\n",
    "        # Lambda*(n-1)\n",
    "        trainer = MLMTrainer(models[-1][1-i], tokenizer, verbalizer)\n",
    "        generated_labels ,_, _  = trainer.evaluate(\n",
    "            T_ij[-1][i], \n",
    "            models[-1][1-i],\n",
    "            n_shots = 10*(1+j),\n",
    "            return_generated_dataset=True\n",
    "        )\n",
    "\n",
    "        logging.info(\"Ended generating labels\")\n",
    "        T_n = deepcopy(train_datas)\n",
    "        for j_i in range(len(T_n[i])):\n",
    "            for k in range(len(T_n[i][j_i])):\n",
    "                if j_i < len(generated_labels):\n",
    "                    T_n[i][j_i][k]['label'] = generated_labels[j_i][k]\n",
    "        new_T.append(T_n[i])\n",
    "\n",
    "    T_ij.append(new_T)\n",
    "\n",
    "    logging.info(f\"Training models with {(j+1)*5} shots\")\n",
    "    # train\n",
    "    models.append([\n",
    "        AutoModelForMaskedLM.from_pretrained(f\"distilbert-base-uncased\").to(device)\n",
    "        for i in range(2)\n",
    "    ])\n",
    "    for i in range(n_patterns):\n",
    "\n",
    "        best_model = None\n",
    "        best_f1 = 0\n",
    "        trainer = MLMTrainer(\n",
    "            models[-1][i],\n",
    "            tokenizer,\n",
    "            verbalizer\n",
    "        )\n",
    "\n",
    "        trainer.compile(\n",
    "            T_ij[-1][i],\n",
    "            10*(j+1),\n",
    "            n_validation=0 \n",
    "        )\n",
    "\n",
    "        y_true, y_pred = trainer.evaluate(\n",
    "            train_datas[i][100:],\n",
    "            models[-1][i],\n",
    "            tokenizer,\n",
    "            verbalizer,\n",
    "            n_shots = 10\n",
    "        )\n",
    "\n",
    "        f1 = multiclass_f1_score(\n",
    "            y_pred,\n",
    "            y_true\n",
    "        )\n",
    "\n",
    "        logging.info(f\"f1 - score: {f1}\")\n",
    "\n",
    "        if (f1 > best_f1):\n",
    "            best_f1 = f1\n",
    "            best_model = deepcopy(models[-1][i])\n",
    "\n",
    "        best_model.push_to_hub(f\"peulsilva/ipet-model{j+1}-pattern{i}-10-shots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
