{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoModelForSequenceClassification,AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.nn.functional import cross_entropy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from torcheval.metrics.functional import multiclass_f1_score, multiclass_confusion_matrix\n",
    "from copy import deepcopy, copy\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from huggingface_hub import notebook_login\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "import os \n",
    "while 'notebooks' in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "import re\n",
    "from typing import List, Dict\n",
    "    \n",
    "from src.preprocessing.laser.laser_processor import LaserProcessor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘logs’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir logs\n",
    "model_name = \"LASER\"\n",
    "dataset_name = \"FUNSD\"\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "logging.basicConfig(filename=f'logs/{model_name}_{dataset_name}.log', encoding='utf-8', level= logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing model (GPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde52ee7298c45e39cf881aab1868626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# clf = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     \"peulsilva/LASER-CLF-GPT\", \n",
    "#     num_labels=2,\n",
    "# ).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", cache_dir = \"/Data/pedro.silva/\",padding_side = \"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", cache_dir = \"/Data/pedro.silva/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7241732096"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MistralForCausalLM' object has no attribute 'transformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vector \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241m.\u001b[39mwte\u001b[38;5;241m.\u001b[39mweight\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MistralForCausalLM' object has no attribute 'transformer'"
     ]
    }
   ],
   "source": [
    "vector = model.transformer.wte.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvector\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vector' is not defined"
     ]
    }
   ],
   "source": [
    "vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing FUNSD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d0eed7e8144625936818195b61acb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/16.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e46149dcf94beba5d4668ada477d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7999d1017d42f7ab0b507d18869bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"nielsr/funsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-HEADER',\n",
       " 2: 'I-HEADER',\n",
       " 3: 'B-QUESTION',\n",
       " 4: 'I-QUESTION',\n",
       " 5: 'B-ANSWER',\n",
       " 6: 'I-ANSWER'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = train_split\\\n",
    "            .features['ner_tags']\\\n",
    "            .feature\\\n",
    "            .names\n",
    "label_keymap = {k:v for k,v in enumerate(label_names)} \n",
    "label_keymap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32003, 4096)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.add_special_tokens({\n",
    "#     'pad_token': '[PAD]',\n",
    "# })\n",
    "\n",
    "tokenizer.add_tokens([\n",
    "    \"[B]\",\n",
    "    \"[E]\",\n",
    "    \"[T]\"\n",
    "])\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/149 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [00:44<00:00,  3.32it/s]\n"
     ]
    }
   ],
   "source": [
    "laser_data = LaserProcessor(\n",
    "    train_split,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = laser_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    laser_data[0:100],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_chars = set([\"[B]\", \"[E]\", \"[T]\", \"QUESTION\", \"ANSWER\", \"NONE\", \"HEADER\"])\n",
    "tag_ids = []\n",
    "for special_char in special_chars:\n",
    "    if special_char[0] != '[':\n",
    "        special_char = special_char.lower()\n",
    "        \n",
    "    special_char_id = tokenizer.vocab[special_char]\n",
    "    tag_ids.append(special_char_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24115, 17496, 32000, 4983, 8607, 32002, 32001]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DistilBertForMaskedLM' object has no attribute 'transformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.98/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m embeddings \u001b[39m=\u001b[39m model\\\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.98/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m.\u001b[39;49mtransformer\\\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.98/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m.\u001b[39mwte\\\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.98/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m.\u001b[39mweight\\\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.98/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DistilBertForMaskedLM' object has no attribute 'transformer'"
     ]
    }
   ],
   "source": [
    "embeddings = model\\\n",
    "    .transformer\\\n",
    "    .wte\\\n",
    "    .weight\\\n",
    "    .to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1101, -0.0393,  0.0331,  ..., -0.1364,  0.0151,  0.0453],\n",
       "        [ 0.0403, -0.0486,  0.0462,  ...,  0.0861,  0.0025,  0.0432],\n",
       "        [-0.1275,  0.0479,  0.1841,  ...,  0.0899, -0.1297, -0.0879],\n",
       "        ...,\n",
       "        [-0.0013, -0.0267,  0.0183,  ...,  0.0220,  0.0232, -0.0007],\n",
       "        [ 0.0289,  0.0136,  0.0223,  ..., -0.0047, -0.0088, -0.0342],\n",
       "        [-0.0078, -0.0201, -0.0052,  ..., -0.0302, -0.0199, -0.0076]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_from_list(\n",
    "    next_word_predictions : torch.Tensor,\n",
    "    token_ids : List[int],\n",
    "    probability : float\n",
    "):\n",
    "    predictions = []\n",
    "    for word_id in token_ids:\n",
    "        predictions.append(\n",
    "            [next_word_predictions[word_id].item(), word_id],\n",
    "        )\n",
    "\n",
    "    predictions_tensor = torch.Tensor(predictions)\\\n",
    "        .to(device)\n",
    "        # .softmax(dim = 0)\n",
    "\n",
    "    predictions_tensor[:,0] = predictions_tensor[:,0]\\\n",
    "        .softmax(dim = 0)\n",
    "\n",
    "    predictions_tensor[:,0] *= probability\n",
    "\n",
    "    return predictions_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corrected_predictions(\n",
    "    predictions_src : torch.Tensor,\n",
    "    predictions_tag: torch.Tensor,\n",
    "    next_word_predictions : torch.Tensor\n",
    "):\n",
    "    new_logits = torch.zeros_like(next_word_predictions)\n",
    "\n",
    "    for idx, word_id in enumerate(predictions_src[:, 1]):\n",
    "        token_id = int(word_id)\n",
    "        new_logits[token_id] = predictions_src[idx, 0]\n",
    "\n",
    "    for idx, tag_id in enumerate(predictions_tag[: ,1]):\n",
    "        token_id = int(tag_id)\n",
    "        new_logits[token_id] = predictions_tag[idx, 0]\n",
    "\n",
    "    return new_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label(\n",
    "    next_word_predictions : torch.Tensor,\n",
    "    token_label_id : int\n",
    "):\n",
    "    label = torch.zeros_like(next_word_predictions)\n",
    "    label[token_label_id] = 1\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_n_tokens(\n",
    "    n:int,\n",
    "    X: list, \n",
    "    generated_text: list,\n",
    "    use_mask : bool = False\n",
    "):\n",
    "    if use_mask:\n",
    "        all_tokens = (X[0] + \" \".join(generated_text) + \" \" + tokenizer.mask_token).split('-') \n",
    "    else:\n",
    "        all_tokens = (X[0] + \" \".join(generated_text)).split('-')\n",
    "        \n",
    "    last_n_tokens = \" \".join(all_tokens[-n:])\n",
    "    \n",
    "    return last_n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/293 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 293/293 [00:30<00:00,  9.75it/s]\n",
      "100%|██████████| 751/751 [01:20<00:00,  9.28it/s]\n",
      "100%|██████████| 300/300 [00:31<00:00,  9.67it/s]\n",
      "100%|██████████| 1308/1308 [02:20<00:00,  9.32it/s]\n",
      "100%|██████████| 279/279 [00:27<00:00, 10.15it/s]\n",
      "100%|██████████| 426/426 [00:46<00:00,  9.24it/s]\n",
      "100%|██████████| 497/497 [00:53<00:00,  9.26it/s]\n",
      "100%|██████████| 507/507 [00:53<00:00,  9.46it/s]\n",
      "100%|██████████| 430/430 [00:43<00:00,  9.83it/s]\n",
      "100%|██████████| 535/535 [00:54<00:00,  9.77it/s]\n",
      "100%|██████████| 293/293 [00:30<00:00,  9.64it/s]\n",
      "100%|██████████| 751/751 [01:20<00:00,  9.39it/s]\n",
      "100%|██████████| 300/300 [00:30<00:00,  9.69it/s]\n",
      "100%|██████████| 1308/1308 [02:19<00:00,  9.38it/s]\n",
      "100%|██████████| 279/279 [00:27<00:00, 10.22it/s]\n",
      "100%|██████████| 426/426 [00:45<00:00,  9.33it/s]\n",
      "100%|██████████| 497/497 [00:53<00:00,  9.29it/s]\n",
      "100%|██████████| 507/507 [00:53<00:00,  9.48it/s]\n",
      "100%|██████████| 430/430 [00:43<00:00,  9.84it/s]\n",
      "100%|██████████| 535/535 [00:54<00:00,  9.82it/s]\n",
      "100%|██████████| 293/293 [00:28<00:00, 10.36it/s]\n",
      "100%|██████████| 751/751 [01:19<00:00,  9.42it/s]\n",
      "100%|██████████| 300/300 [00:30<00:00,  9.70it/s]\n",
      "100%|██████████| 1308/1308 [02:18<00:00,  9.45it/s]\n",
      "100%|██████████| 279/279 [00:27<00:00, 10.09it/s]\n",
      "100%|██████████| 426/426 [00:45<00:00,  9.36it/s]\n",
      "100%|██████████| 497/497 [00:53<00:00,  9.29it/s]\n",
      "100%|██████████| 507/507 [00:52<00:00,  9.57it/s]\n",
      "100%|██████████| 430/430 [00:44<00:00,  9.73it/s]\n",
      "100%|██████████| 535/535 [00:53<00:00,  9.96it/s]\n",
      "100%|██████████| 293/293 [00:29<00:00, 10.10it/s]\n",
      "100%|██████████| 751/751 [01:20<00:00,  9.36it/s]\n",
      "100%|██████████| 300/300 [00:30<00:00,  9.72it/s]\n",
      "100%|██████████| 1308/1308 [02:17<00:00,  9.50it/s]\n",
      "100%|██████████| 279/279 [00:27<00:00, 10.26it/s]\n",
      "100%|██████████| 426/426 [00:44<00:00,  9.53it/s]\n",
      "100%|██████████| 497/497 [00:53<00:00,  9.34it/s]\n",
      "100%|██████████| 507/507 [00:53<00:00,  9.52it/s]\n",
      "100%|██████████| 430/430 [00:44<00:00,  9.67it/s]\n",
      "100%|██████████| 535/535 [00:55<00:00,  9.57it/s]\n",
      "100%|██████████| 293/293 [00:30<00:00,  9.62it/s]\n",
      "100%|██████████| 751/751 [01:20<00:00,  9.37it/s]\n",
      "100%|██████████| 300/300 [00:30<00:00,  9.81it/s]\n",
      "100%|██████████| 1308/1308 [02:18<00:00,  9.44it/s]\n",
      "100%|██████████| 279/279 [00:27<00:00, 10.07it/s]\n",
      "100%|██████████| 426/426 [00:45<00:00,  9.37it/s]\n",
      "100%|██████████| 497/497 [00:52<00:00,  9.40it/s]\n",
      "100%|██████████| 507/507 [00:52<00:00,  9.62it/s]\n",
      "100%|██████████| 430/430 [00:43<00:00,  9.91it/s]\n",
      "100%|██████████| 535/535 [00:52<00:00, 10.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# model = GPT2LMHeadModel.from_pretrained(\n",
    "#     \"gpt2\",\n",
    "#     output_hidden_states =True\n",
    "# ).to(device)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr = 1e-4\n",
    ")\n",
    "\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in (range(n_epochs)):\n",
    "    losses = []\n",
    "    for [X, y] in (dataloader):\n",
    "        generated_text = []\n",
    "        \n",
    "        label_tokens = tokenizer(\n",
    "            y, \n",
    "        ).input_ids[0]\n",
    "            \n",
    "        src =  np.unique(tokenizer(X[0], truncation=True)['input_ids'])\n",
    "        for label_token in tqdm(label_tokens):\n",
    "            \n",
    "            batch = tokenizer(\n",
    "                get_last_n_tokens(256, X, generated_text),\n",
    "                truncation= True,\n",
    "                padding= \"max_length\",\n",
    "                return_tensors= \"pt\",\n",
    "                max_length=256,\n",
    "            )\n",
    "\n",
    "            \n",
    "            with torch.no_grad():\n",
    "\n",
    "                clf_batch = tokenizer(\n",
    "                    get_last_n_tokens(32, X, generated_text),\n",
    "                    truncation= True,\n",
    "                    padding= \"max_length\",\n",
    "                    return_tensors= \"pt\",\n",
    "                    max_length=32,\n",
    "                )\n",
    "\n",
    "                p_k = clf(**clf_batch)\\\n",
    "                    .logits\\\n",
    "                    .softmax(dim = 1)[0,1]\\\n",
    "                    .item()\n",
    "                \n",
    "            outputs = model(**batch)\n",
    "            next_word_predictions = outputs[0][0,-1,:]        \n",
    "\n",
    "            predictions_src = get_predictions_from_list(\n",
    "                next_word_predictions,\n",
    "                src,\n",
    "                p_k\n",
    "            )\n",
    "            # predictions_src.requires_grad = True\n",
    "                \n",
    "            predictions_tag = get_predictions_from_list(\n",
    "                next_word_predictions,\n",
    "                tag_ids, \n",
    "                1-p_k\n",
    "            )\n",
    "            # predictions_tag.requires_grad = True\n",
    "\n",
    "            idx, max_proba_tag = predictions_tag[:, 0].argmax(), predictions_tag[:, 0].max()\n",
    "            best_tag_word_id = predictions_tag[idx,1]\\\n",
    "                .item()\n",
    "            best_tag_word_id = int(best_tag_word_id)\n",
    "\n",
    "            idx, max_proba_src = predictions_src[:, 0].argmax(), predictions_src[:, 0].max()\n",
    "            best_src_word_id = predictions_src[idx, 1]\\\n",
    "                .item()\n",
    "            best_src_word_id = int(best_src_word_id)\n",
    "\n",
    "            if max_proba_src > max_proba_tag:\n",
    "                next_word = tokenizer.decode(best_src_word_id)\n",
    "            \n",
    "            else:\n",
    "                next_word = tokenizer.decode(best_tag_word_id)\n",
    "            \n",
    "            generated_text.append(next_word)\n",
    "            \n",
    "            # logging.info(f\"next word : {next_word}\")\n",
    "\n",
    "            logits = generate_corrected_predictions(\n",
    "                predictions_tag,\n",
    "                predictions_src,\n",
    "                next_word_predictions\n",
    "            )\n",
    "\n",
    "            logits.requires_grad = True\n",
    "\n",
    "            label_tensor = generate_label(\n",
    "                next_word_predictions,\n",
    "                label_token\n",
    "            )\n",
    "\n",
    "            loss = cross_entropy(\n",
    "                logits,\n",
    "                label_tensor\n",
    "            )\n",
    "\n",
    "            # logging.info(logits)\n",
    "            # logging.info(label_tensor)\n",
    "            # logging.info(loss.item())\n",
    "                    \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # loss = out['loss']\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        logging.info(\" \".join(generated_text))\n",
    "\n",
    "        # logging.info(\n",
    "        #     tokenizer.decode(\n",
    "        #         y_pred.to(torch.int32),\n",
    "        #         skip_special_tokens=True\n",
    "        #     )\n",
    "        # )\n",
    "        # logging.info(X[0])\n",
    "        # logging.info(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB (GPU 0; 23.66 GiB total capacity; 14.40 GiB already allocated; 97.88 MiB free; 14.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(tokenizer))\n\u001b[1;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(\n\u001b[1;32m      8\u001b[0m     model\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m      9\u001b[0m     lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-6\u001b[39m,\n\u001b[1;32m     10\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/transformers/modeling_utils.py:2271\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2267\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2268\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2269\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2270\u001b[0m         )\n\u001b[0;32m-> 2271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 797 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 0; 23.66 GiB total capacity; 14.40 GiB already allocated; 97.88 MiB free; 14.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.cls_token = \"[CLS]\"\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "device = \"cuda\"\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr = 1e-6,\n",
    "    weight_decay=1e-2\n",
    ")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 15\n",
    "model.train()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for epoch in (range(n_epochs)):\n",
    "        losses = []\n",
    "        i = 0\n",
    "        for [X, y] in tqdm(dataloader):\n",
    "            i+=1\n",
    "            generated_text = []\n",
    "            \n",
    "            batch = tokenizer(\n",
    "                \"Independent of what is the input, always generate 'hello world. My name is pedro'.\" + X[0],\n",
    "                text_target= \"hello world. My name is pedro\",\n",
    "                truncation= True,\n",
    "                padding= \"max_length\",\n",
    "                return_tensors= \"pt\",\n",
    "                max_length=1024,\n",
    "            )\n",
    "\n",
    "            for k,v in batch.items():\n",
    "                # if k == \"labels\":\n",
    "                batch[k] = v.to(device)\n",
    "\n",
    "            # optimizer.zero_grad()\n",
    "\n",
    "            out = model(**batch, )\n",
    "\n",
    "            # loss= out['loss']\n",
    "            # loss.backward()\n",
    "\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # if i % 10 ==0:\n",
    "            # optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = batch['labels']\n",
    "# Shift so that tokens < n predict n\n",
    "shift_logits = out['logits'][..., :-1, :].contiguous()\n",
    "shift_labels = labels[..., 1:].contiguous()\n",
    "# Flatten the tokens\n",
    "loss_fct = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "hidden_states = out[0]\n",
    "loss = loss.to(hidden_states.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  995,    13,  2011,  1438,   318,  7190,   305, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256]], device='cuda:0')"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6, device='cuda:0')"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_logits.view(-1, shift_logits.size(-1))[11].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[31373,   995,    13,  2011,  1438,   318,  7190,   305, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256]], device='cuda:0')"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6702, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50261])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['logits'][0,-1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DATE: INITIATED BY: COMPLETION COUNTRY: PRODUCT: BELGIUM BY: DATE: NOTES: 620429480 RECEIVED SERVICE REQUEST FROM R & D BY INTERNATIONAL LICENSEE OPERATIONS REQUEST NO.: 25- 84 March 27, 1984 P. H. HARPER PHH TARGET DATE: April 13, 1984 LUCKY STRIKE Filter and VICEROY NATURE OF WORK: Advise if locally obtained Yucatan Honey (sample enclosed) is an acceptable substitute for HALWAY. R & D COMMENTS: Target the P A 29Mar 84 a) Nature of work should be specified in exact terms. b) R & D should advise if completion date cannot be met. c) Two copies of this form to be sent to R & D by initiator and R & D is to return to T. O. one completed copy. MH/ enm 0036/ (r) # 2894M APR 2 1984 P. H. H.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>[E]'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_ = tokenizer(\n",
    "    X[0] ,\n",
    "    # text_target=y[0],\n",
    "    truncation= True,\n",
    "    padding= \"max_length\",\n",
    "    return_tensors= \"pt\",\n",
    "    max_length=256,\n",
    ")\n",
    "for k,v in batch_.items():\n",
    "    batch_[k] = v.to(device)\n",
    "\n",
    "    \n",
    "gen_text = model.generate(\n",
    "    **batch_,\n",
    "    max_length=150,  \n",
    "    # num_return_sequences=5,\n",
    "    # no_repeat_ngram_size=2,\n",
    "    # repetition_penalty=1.5,\n",
    "    # top_p=0.92,\n",
    "    # temperature=.85,\n",
    "    # do_sample=True,\n",
    "    # top_k=125,\n",
    "    # early_stopping=True\n",
    ")\n",
    "\n",
    "tokenizer.decode(gen_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, device = \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"DATE: DEPARTMENT: Type EQPR B&W QUALITY Implement: Date 632120763 QUALITY IMPROVEMENT SUGGESTION OUR MISSION IS SUPERIOR CONSUMER SATISFACTION Highest Quality. Teamwork, Do Right Things Right The First Time RESEARCH & DEVELOPMENT Quality Coord Only June 21, 1993 R&D Library Carol S. Lincoln 407- 64- 3484 SUBMITTED BY: SUBMITTER'S SS#: Date Rec'd QIP Log #1 Status (1993) Keywords (1993) 6/ 21/ 93 93- 0301 SUGGESTION: (Describe Current Situation and Idea) The current system of managing records is too complex. The trend seems to be increasingly specific, when we should be getting more general. Right now, people must work to understand the system. We must spend too much time adninistering the system, labeling and cleaning our files. Complying is a real burden, both for the individual and for the records coordinators. Describe Possible Solutions And Benefits 1. Drop the category specifications altogether. 2. Use moregeneral categories. hote: I have passed this to Scott Appleton for the task to use in streamlining the records management force program. PROFIT IMPROVEMENT: (If Applicable) RESPOND HERE ☐ Yes ☐ No ☐ Pending Approval Signature/ Date Responder/ Date Sign & Print Names) (See Instructions On Back)\"}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50261. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50261. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      " 10%|█         | 1/10 [00:22<03:22, 22.45s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb Cell 28\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.97/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m word\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39m[\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.97/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m     word \u001b[39m=\u001b[39m word\u001b[39m.\u001b[39mlower()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.97/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m word_token \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mvocab[word]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.97/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39mif\u001b[39;00m word\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m target_text\u001b[39m.\u001b[39mlower():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.97/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m     real_labels\u001b[39m.\u001b[39mappend(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/transformers/tokenization_utils_fast.py:181\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvocab\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]:\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_vocab()\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/transformers/tokenization_utils_fast.py:177\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.get_vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_vocab\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]:\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mget_vocab(with_added_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "    'gpt2',\n",
    "    output_hidden_states =True\n",
    ")\n",
    "\n",
    "clf = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"peulsilva/LASER-CLF-GPT\", \n",
    "    num_labels=2,\n",
    ").to(device)\n",
    "\n",
    "device = \"cuda\"\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "clf.resize_token_embeddings(len(tokenizer))\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr = 1e-5\n",
    ")\n",
    "clf.to(device)\n",
    "model.to(device)\n",
    "\n",
    "text = []\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "for epoch in (range(n_epochs)):\n",
    "    for [X, y] in tqdm(dataloader):\n",
    "        target_text = None\n",
    "        generated_text = []\n",
    "        \n",
    "        if type(y) == tuple:\n",
    "            y = y[0]\n",
    "\n",
    "        if type(X) == tuple:\n",
    "            X = X[0]\n",
    "\n",
    "        in_stack = deque(X.split(' '), maxlen=16)\n",
    "        out_stack = deque(y.split(' '))\n",
    "        while len(out_stack)> 0:\n",
    "\n",
    "            if target_text == None:\n",
    "                target_text = \"[B]\"\n",
    "\n",
    "            batch = tokenizer(\n",
    "                \" \".join(in_stack),\n",
    "                truncation= True,\n",
    "                padding= \"max_length\",\n",
    "                return_tensors= \"pt\",\n",
    "                max_length=32,\n",
    "            )\n",
    "\n",
    "\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.to(device)\n",
    "\n",
    "            is_in_src = clf(**batch)\\\n",
    "                .logits\\\n",
    "                .squeeze()\\\n",
    "                .argmax()\n",
    "            \n",
    "            if is_in_src:\n",
    "                \n",
    "                generated_text.append(target_text)\n",
    "                target_text = out_stack.popleft()\n",
    "                in_stack.append(target_text)\n",
    "                continue\n",
    "            \n",
    "            else:\n",
    "                output = model.generate(**batch)\n",
    "                next_token_logits = output.logits[0, :, :]\n",
    "\n",
    "                logits = []\n",
    "                real_labels = []\n",
    "                for word in special_chars:\n",
    "                    # proba = \n",
    "                    if word.startswith(\"[\"):\n",
    "                        continue\n",
    "                    \n",
    "                    if not word.startswith('['):\n",
    "                        word = word.lower()\n",
    "                        \n",
    "                    word_token = tokenizer.vocab[word]\n",
    "                    if word.lower() == target_text.lower():\n",
    "                        real_labels.append(1)\n",
    "                    \n",
    "                    else:\n",
    "                        real_labels.append(0)\n",
    "                    \n",
    "                    logits.append([\n",
    "                        word_token,\n",
    "                        next_token_logits[-1, word_token].item()\n",
    "                    ])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                logits = torch.tensor(logits, requires_grad=True).to(device)\n",
    "\n",
    "                ce_loss = cross_entropy(\n",
    "                    logits[:,1].softmax(dim = 0).to(torch.float64),\n",
    "                    torch.tensor(real_labels)\\\n",
    "                        .to(torch.float64)\\\n",
    "                        .to(device),\n",
    "                )\n",
    "                                \n",
    "                generated_text.append(\n",
    "                    tokenizer.decode(\n",
    "                        int(logits.max(dim=0)[0][0])\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                in_stack.append(\n",
    "                    tokenizer.decode(\n",
    "                        int(logits.max(dim=0)[0][0])\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                \n",
    "                ce_loss.backward()\n",
    "                \n",
    "                optimizer.step()\n",
    "\n",
    "                target_text = out_stack.popleft()\n",
    "                \n",
    "        text.append(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_words = list(special_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in force_words:\n",
    "    if not word[0] == \"[\":\n",
    "        word = word.lower()\n",
    "        if word.upper() in force_words:\n",
    "            force_words.remove(word.upper())\n",
    "            force_words.append(word.lower())\n",
    "force_ids= tokenizer(force_words, add_special_tokens= False).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[E]', '[T]', '[B]', 'question', 'header', 'answer', 'none']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "force_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HEADER'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[50259], [50260], [50258], [25652], [25677], [41484], [23108]]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "force_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 15.71 GiB total capacity; 15.00 GiB already allocated; 17.75 MiB free; 15.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.97/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.97/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     batch[k] \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.97/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.97/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.97/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     num_beams \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.97/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     force_words_ids \u001b[39m=\u001b[39;49m force_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.97/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# early_stopping = True,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.97/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# max_new_tokens = 100,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.97/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# do_sample = True,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.97/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# top_p = 0.95,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.97/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# top_k = 10,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.97/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# renormalize_logits =True,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.97/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     no_repeat_ngram_size \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.97/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# max_length = 20,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.97/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m# min_new_tokens = 100\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B129.104.254.97/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/notebooks/few-shot-learning/laser/02-laser.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m )\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/transformers/generation/utils.py:1825\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1818\u001b[0m input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1819\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1820\u001b[0m     expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1821\u001b[0m     is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1822\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1823\u001b[0m )\n\u001b[1;32m   1824\u001b[0m \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1825\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstrained_beam_search(\n\u001b[1;32m   1826\u001b[0m     input_ids,\n\u001b[1;32m   1827\u001b[0m     constrained_beam_scorer\u001b[39m=\u001b[39;49mconstrained_beam_scorer,\n\u001b[1;32m   1828\u001b[0m     logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1829\u001b[0m     stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1830\u001b[0m     pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1831\u001b[0m     eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1832\u001b[0m     output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1833\u001b[0m     return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1834\u001b[0m     synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1835\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1836\u001b[0m )\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/transformers/generation/utils.py:4061\u001b[0m, in \u001b[0;36mGenerationMixin.constrained_beam_search\u001b[0;34m(self, input_ids, constrained_beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   4057\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   4059\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 4061\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   4062\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   4063\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   4064\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   4065\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   4066\u001b[0m )\n\u001b[1;32m   4068\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   4069\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1076\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1076\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1077\u001b[0m     input_ids,\n\u001b[1;32m   1078\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1079\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1080\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1081\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1082\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1083\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1084\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1085\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1086\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1087\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1088\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1089\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1090\u001b[0m )\n\u001b[1;32m   1091\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1093\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:900\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    890\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    891\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    892\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    897\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    898\u001b[0m     )\n\u001b[1;32m    899\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 900\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    901\u001b[0m         hidden_states,\n\u001b[1;32m    902\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    903\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    904\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    905\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    906\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    907\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    908\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    909\u001b[0m     )\n\u001b[1;32m    911\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    912\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 390\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    391\u001b[0m     hidden_states,\n\u001b[1;32m    392\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    393\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    394\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    395\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    396\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    397\u001b[0m )\n\u001b[1;32m    398\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:331\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    329\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    330\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    333\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_heads(attn_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    334\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:186\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    183\u001b[0m attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(query, key\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_attn_weights:\n\u001b[0;32m--> 186\u001b[0m     attn_weights \u001b[39m=\u001b[39m attn_weights \u001b[39m/\u001b[39;49m torch\u001b[39m.\u001b[39;49mfull(\n\u001b[1;32m    187\u001b[0m         [], value\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m \u001b[39m0.5\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mattn_weights\u001b[39m.\u001b[39;49mdtype, device\u001b[39m=\u001b[39;49mattn_weights\u001b[39m.\u001b[39;49mdevice\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[39m# Layer-wise attention scaling\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_attn_by_inverse_layer_idx:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 15.71 GiB total capacity; 15.00 GiB already allocated; 17.75 MiB free; 15.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "batch = tokenizer(\n",
    "    X,\n",
    "    truncation= True,\n",
    "    padding= \"max_length\",\n",
    "    return_tensors= \"pt\",\n",
    "    max_length=512,\n",
    "            \n",
    ")\n",
    "for k, v in batch.items():\n",
    "    batch[k] = v.to(device)\n",
    "\n",
    "out = model.generate(\n",
    "    **batch,\n",
    "    num_beams = 10,\n",
    "    force_words_ids = force_ids,\n",
    "    # early_stopping = True,\n",
    "    # max_new_tokens = 100,\n",
    "    # do_sample = True,\n",
    "    # top_p = 0.95,\n",
    "    # top_k = 10,\n",
    "    # renormalize_logits =True,\n",
    "    no_repeat_ngram_size = 1,\n",
    "    # max_length = 20,\n",
    "    # min_new_tokens = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 513])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title OVERALL: SEX: AGE: BRAND SMOKER Male 465508326 Female 6 7 (17) (205) 3.1 4.2 (129) (95) 5 9 (117) (105) (63) (66) (46) (49) 3.2 3.0 4.4 4.0 1.9 5.0 (104) (120) 7 7 (101) (121) 3.4% (224) 7% (222) BELAIR 285 % % SCORE BASE SCORE BASE PM6 COMMENTS Brand NEWSPAPER SCORES AUDIENCE STUDIES \" KALEIDOSCOPE - - GONE WITH THE WIND \" * Project # Total Sample 72- 31 Code # Type of Ad Newspaper (Date) R/ BSS- 71- 19 PARADE, 1 Page, 4- Color PROVED RECALL *This was a combination ad with RALEIGH. Under 25 25- 34 35- 44 45 & Over Under 35 35 & Over Test Brand Smokers All Other Smokers Pittsburgh Pittsburgh PUTSBURGH PRESS (4 /23 /72 San Diego SAN DIEGO UNION (4 /23 /72 Dayton DAYTON NEWS (4 /23 /72) Birmingham BIRMINGHAM NEWS (4 /23 /72)\\uf702[E]'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out[0],\n",
    "                 skip_special_tokens= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[41484,  3280,  3280,  3280,  3280,  3280,  3280,  3280,  3280,  3280,\n",
       "          3280,  3280,  3280,  3280,  3280,  3280, 50257, 50257, 50257, 50257,\n",
       "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "         50257, 50257]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = tokenizer(\n",
    "    \" \".join(in_stack),\n",
    "    \n",
    "    truncation= True,\n",
    "    padding= \"max_length\",\n",
    "    return_tensors= \"pt\",\n",
    "    max_length=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**batch)\n",
    "next_token_logits = output.logits[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50261])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = []\n",
    "real_labels = []\n",
    "for word in special_chars:\n",
    "    if not word.startswith('['):\n",
    "        word = word.lower()\n",
    "        \n",
    "    word_token = tokenizer.vocab[word]\n",
    "    if word.lower() == target_text.lower():\n",
    "        real_labels.append(1)\n",
    "    \n",
    "    else:\n",
    "        real_labels.append(0)\n",
    "    \n",
    "    logits.append([\n",
    "        word_token,\n",
    "        next_token_logits[-1, word_token].item()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor(logits, requires_grad=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.5677e+04, -8.7671e+01],\n",
       "        [ 5.0260e+04, -4.7957e+00],\n",
       "        [ 2.3108e+04, -8.8531e+01],\n",
       "        [ 5.0259e+04, -5.7342e+00],\n",
       "        [ 2.5652e+04, -9.0468e+01],\n",
       "        [ 4.1484e+04, -9.2401e+01],\n",
       "        [ 5.0258e+04,  1.5480e+00]], device='cuda:0',\n",
       "       grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2.5677e+04, -8.7671e+01],\n",
       "         [ 5.0260e+04, -4.7957e+00],\n",
       "         [ 2.3108e+04, -8.8531e+01],\n",
       "         [ 5.0259e+04, -5.7342e+00],\n",
       "         [ 2.5652e+04, -9.0468e+01],\n",
       "         [ 4.1484e+04, -9.2401e+01],\n",
       "         [ 5.0258e+04,  1.5480e+00]], device='cuda:0',\n",
       "        grad_fn=<ToCopyBackward0>),\n",
       " [0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, real_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque(['Total',\n",
       "       'Pressure',\n",
       "       'Drop',\n",
       "       '(encap.)',\n",
       "       '[E]',\n",
       "       'QUESTION',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       'Tipping',\n",
       "       'Length',\n",
       "       '[E]',\n",
       "       'QUESTION',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       'Print',\n",
       "       'Position',\n",
       "       '(from',\n",
       "       'filter',\n",
       "       'end)',\n",
       "       '[E]',\n",
       "       'QUESTION',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       'Moisture',\n",
       "       'content',\n",
       "       '(Packing)',\n",
       "       '[E]',\n",
       "       'QUESTION',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       'Filter',\n",
       "       'Ventilation',\n",
       "       'Rate',\n",
       "       '[E]',\n",
       "       'QUESTION',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       'Total',\n",
       "       'Cigarette',\n",
       "       'Weight',\n",
       "       '[E]',\n",
       "       'QUESTION',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       'Net',\n",
       "       'Net',\n",
       "       'Tobacco',\n",
       "       '[E]',\n",
       "       'QUESTION',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       'Tobacco',\n",
       "       'Rod',\n",
       "       'Density',\n",
       "       '[E]',\n",
       "       'QUESTION',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       'Tipping',\n",
       "       'and',\n",
       "       'Tipping',\n",
       "       'Application',\n",
       "       '[E]',\n",
       "       'HEADER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       'Tipping',\n",
       "       'Paper:',\n",
       "       '[E]',\n",
       "       'HEADER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       'Supplier',\n",
       "       'Code',\n",
       "       'No(s).',\n",
       "       '[E]',\n",
       "       'QUESTION',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       'Perforation',\n",
       "       'Type',\n",
       "       '&',\n",
       "       'No.',\n",
       "       'of',\n",
       "       'lines',\n",
       "       '[E]',\n",
       "       'QUESTION',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       'Print',\n",
       "       'Description',\n",
       "       '[E]',\n",
       "       'QUESTION',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       'Bobbin',\n",
       "       'Width',\n",
       "       '[E]',\n",
       "       'QUESTION',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       'Dobbin',\n",
       "       'Length',\n",
       "       '[E]',\n",
       "       'QUESTION',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       'ECUSTA/',\n",
       "       'B&W',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       'ECUSTA/',\n",
       "       'B',\n",
       "       '&',\n",
       "       'W',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       'Non',\n",
       "       'Porous',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '64',\n",
       "       'mm',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '2500',\n",
       "       'M',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '36',\n",
       "       'nm/',\n",
       "       'M2',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       'Non',\n",
       "       'Porous',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '64',\n",
       "       'mm',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '2500',\n",
       "       'M',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '36',\n",
       "       'nm',\n",
       "       '/47M2',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '243.6',\n",
       "       'mg/',\n",
       "       'cc',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '243.6',\n",
       "       'mg/',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '858',\n",
       "       'mg',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '858',\n",
       "       'mg',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '35',\n",
       "       'mm',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '32',\n",
       "       'mm',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '32',\n",
       "       'mm',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '35',\n",
       "       'mm',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '13',\n",
       "       '%',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '24.8',\n",
       "       'mm',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '58.5',\n",
       "       'mm',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '24.8',\n",
       "       'mm',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '58.5',\n",
       "       'mm',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '62',\n",
       "       'mm',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '27',\n",
       "       'mm',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '72',\n",
       "       'mm',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '99',\n",
       "       'mm',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '99',\n",
       "       'mm',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '72',\n",
       "       'mm',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]',\n",
       "       '[B]',\n",
       "       '27',\n",
       "       'mm',\n",
       "       '[E]',\n",
       "       'ANSWER',\n",
       "       '[T]'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[E]',\n",
       " '[T]',\n",
       " '[T]',\n",
       " '[T]',\n",
       " '[T]',\n",
       " 'Rod',\n",
       " 'Length',\n",
       " '[E]',\n",
       " '[T]',\n",
       " '[T]',\n",
       " '[T]',\n",
       " '[T]',\n",
       " 'Plug',\n",
       " 'Length',\n",
       " '[E]',\n",
       " '[T]',\n",
       " '[T]',\n",
       " '[T]',\n",
       " '[T]',\n",
       " 'Plug',\n",
       " 'Pressure',\n",
       " 'Drop',\n",
       " '(unencap.)',\n",
       " '[E]',\n",
       " '[T]',\n",
       " '[T]',\n",
       " '[T]',\n",
       " '[T]',\n",
       " 'Plug',\n",
       " 'Pressure',\n",
       " 'Drop',\n",
       " '(encap.)',\n",
       " '[T]',\n",
       " '[T]',\n",
       " '[T]',\n",
       " '[T]',\n",
       " '[T]',\n",
       " 'Circumference',\n",
       " '[E]',\n",
       " '[T]',\n",
       " '[T]',\n",
       " '[T]',\n",
       " '[T]',\n",
       " 'Pressure',\n",
       " 'Drop',\n",
       " '(unencap.)',\n",
       " '[E]',\n",
       " '[T]',\n",
       " '[T]',\n",
       " '[T]']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
