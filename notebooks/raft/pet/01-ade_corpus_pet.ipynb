{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.nn.functional import cross_entropy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from torcheval.metrics.functional import multiclass_f1_score, multiclass_confusion_matrix, binary_f1_score\n",
    "from copy import deepcopy, copy\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from huggingface_hub import notebook_login\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import defaultdict, deque\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
    "\n",
    "import os \n",
    "while 'notebooks' in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "import re\n",
    "from typing import List, Dict\n",
    "    \n",
    "from src.preprocessing.sequence_classification.dataset import get_n_shots_per_class\n",
    "from src.model.mlm.pet_for_text_classification import train \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘logs’: File exists\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "!mkdir logs\n",
    "model_name = \"PET\"\n",
    "dataset_name = \"RAFT-Ade-corpus\"\n",
    "logging.basicConfig(filename=f'logs/{model_name}_{dataset_name}.log', encoding='utf-8', level= logging.INFO)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ade_corpus_dataset = load_dataset(\n",
    "    \"ought/raft\", \n",
    "    name=\"ade_corpus_v2\" , \n",
    "    cache_dir= \"/Data/pedro.silva\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ade_corpus_dataset['train'].add_column(\n",
    "    \"labels\",\n",
    "    list(map(lambda x: 0 if x == 2 else 1, ade_corpus_dataset['train']['Label']))\n",
    ")\n",
    "\n",
    "test_dataset = ade_corpus_dataset['test'].add_column(\n",
    "    \"labels\",\n",
    "    list(map(lambda x: 0 if x == 2 else 1, ade_corpus_dataset['test']['Label']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = np.bincount(train_dataset['labels'])\n",
    "num_classes = len(class_counts)\n",
    "total_samples = len(train_dataset['labels'])\n",
    "\n",
    "class_weights = []\n",
    "for count in class_counts:\n",
    "    weight = 1 / (count / total_samples)\n",
    "    class_weights.append(weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.4285714285714286, 3.3333333333333335], array([35, 15]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights, class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.7\n",
    "def stratified_train_test_split(\n",
    "    dataset : Dataset,\n",
    "    classes : np.ndarray,\n",
    "    train_size : float\n",
    "):\n",
    "    \"\"\"Performs train test split keeping class distributions\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): _description_\n",
    "        classes (np.ndarray): _description_\n",
    "        train_size (float): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \n",
    "\n",
    "    indexes_dict = {}\n",
    "    for label in classes[0]:\n",
    "        indexes_dict[label] = []\n",
    "\n",
    "    for i in range(len(dataset['labels'])):\n",
    "        label = dataset['labels'][i]\n",
    "        text = dataset['Sentence'][i]\n",
    "        indexes_dict[label].append(text)\n",
    "\n",
    "\n",
    "    train_data = {\n",
    "        'labels': [],\n",
    "        'text': []\n",
    "    }\n",
    "\n",
    "    validation_data = {\n",
    "        \"labels\" : [],\n",
    "        \"text\": []\n",
    "    }\n",
    "\n",
    "    # generating train data\n",
    "    for label in classes[0]:\n",
    "        n = len(indexes_dict[label])\n",
    "        size = int(train_size * n)\n",
    "\n",
    "        train_data['text'] += indexes_dict[label][:size]\n",
    "        train_data['labels'] += [label]*size\n",
    "        \n",
    "        validation_data['text'] +=indexes_dict[label][size:]\n",
    "        validation_data['labels'] += [label]* (n-size)\n",
    "\n",
    "    return train_data, validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(train_dataset['labels'], return_counts=True)\n",
    "train_data, val_data = stratified_train_test_split(\n",
    "    train_dataset,\n",
    "    classes,\n",
    "    0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern1(text: str, tokenizer : AutoTokenizer):\n",
    "    return f\"{text} Are any there adverse drug effects ? {tokenizer.mask_token}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-large\",\n",
    "                                          cache_dir = \"/Data/pedro.silva\")\n",
    "idx2class = {\n",
    "    0: \"no\",\n",
    "    1: \"yes\"\n",
    "}\n",
    "\n",
    "class2idx = {\n",
    "    \"no\": 0,\n",
    "    \"yes\": 1\n",
    "}\n",
    "\n",
    "class_names = ['no', 'yes']\n",
    "\n",
    "verbalizer = {idx : tokenizer.vocab[x.lower()] for idx, x in enumerate(class_names)}\n",
    "inverse_verbalizer = {tokenizer.vocab[x.lower()] : idx for idx, x in enumerate(class_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2362, 1: 10932}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text : List[str], labels : List[int]):\n",
    "\n",
    "    processed_text = []\n",
    "    processed_labels = []\n",
    "    for idx in range(len(text)):\n",
    "        label = idx2class[labels[idx]]\n",
    "        text_ = text[idx]\n",
    "\n",
    "        processed_text.append(pattern1(text_, tokenizer))\n",
    "        processed_labels.append(label)\n",
    "\n",
    "    return processed_text, processed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, train_labels = preprocess(train_data['text'], train_data['labels'])\n",
    "val_text, val_labels = preprocess(val_data['text'], val_data['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No regional side effects were noted. Are any there adverse drug effects ? <mask>',\n",
       " 'We describe the case of a 10-year-old girl with two epileptic seizures and subcontinuous spike-waves during sleep, who presented unusual side-effects related to clobazam (CLB) monotherapy. Are any there adverse drug effects ? <mask>',\n",
       " 'The INR should be monitored more frequently when bosentan is initiated, adjusted, or discontinued in patients taking warfarin. Are any there adverse drug effects ? <mask>',\n",
       " 'As termination was not an option for the family, the patient was extensively counseled and treated with oral ganciclovir. Are any there adverse drug effects ? <mask>',\n",
       " 'Pulses have been given for periods up to three years without evident toxicity. Are any there adverse drug effects ? <mask>',\n",
       " 'CONCLUSION: Pancreatic enzyme intolerance, although rare, would be a major problem in the management of patients with CF. Are any there adverse drug effects ? <mask>',\n",
       " 'The treatment of Toxoplasma encephalitis in patients with acquired immunodeficiency syndrome. Are any there adverse drug effects ? <mask>',\n",
       " 'A challenge with clozapine was feasible and showed no clinical symptoms of eosinophilia. Are any there adverse drug effects ? <mask>',\n",
       " 'These results indicate that the hyponatremia in this case was due to SIADH and that SIADH was caused by an increased release of vasopressin probably because of the antiviral drug (acyclovir) or infection of varicella zoster virus (VZV) in a single dermatome. Are any there adverse drug effects ? <mask>',\n",
       " 'In 1991 the patient were found to be seropositive for HCV antibodies as detected by the ELISA method and confirmed by the RIBA method. Are any there adverse drug effects ? <mask>',\n",
       " 'MRI has a high sensitivity and specificity in the diagnosis of osteonecrosis and should be used when this condition is suspected. Are any there adverse drug effects ? <mask>',\n",
       " 'Treatment of silastic catheter-induced central vein septic thrombophlebitis. Are any there adverse drug effects ? <mask>',\n",
       " 'These organisms have occasionally been reported as a cause of serious infections in man but have not been reported as a cause of shunt infection. Are any there adverse drug effects ? <mask>',\n",
       " 'NEH must be considered in lupus patients receiving cytotoxic agents to avoid inappropriate use of corticosteroids or antibiotics in this self-limited condition. Are any there adverse drug effects ? <mask>',\n",
       " 'The patient had no skin reactions for the next 12 mo, with the exception of injection-site papules. Are any there adverse drug effects ? <mask>',\n",
       " 'Of the 16 patients, including the 1 reported here, only 3 displayed significant shortening of the agranulocytic period after treatment. Are any there adverse drug effects ? <mask>',\n",
       " 'A closer look at septic shock. Are any there adverse drug effects ? <mask>',\n",
       " 'A 24- to 48-h course of large-dose glucocorticoid therapy is often used in the acute management of spinal cord injury. Are any there adverse drug effects ? <mask>',\n",
       " 'CT-scan disclosed right ethmoid sinusitis that spread to the orbit after surgery. Are any there adverse drug effects ? <mask>',\n",
       " 'No abnormalities were identified on review of collection and processing records. Are any there adverse drug effects ? <mask>',\n",
       " 'A case study is presented of a licensed practical nurse who developed persistent contact dermatitis. Are any there adverse drug effects ? <mask>',\n",
       " 'After the first oral dose of propranolol, syncope developed together with atrioventricular block. Are any there adverse drug effects ? <mask>',\n",
       " 'OBJECTIVE: To describe onset of syndrome of inappropriate antidiuretic hormone (SIADH) associated with vinorelbine therapy for advanced breast cancer. Are any there adverse drug effects ? <mask>',\n",
       " 'Macular infarction after endophthalmitis treated with vitrectomy and intravitreal gentamicin. Are any there adverse drug effects ? <mask>',\n",
       " 'These cases were considered unusual in light of the short delay of their onset after initiation of immunosuppressive therapy and their fulminant course: 3 of these patients died of PCP occurring during the first month of treatment with prednisone. Are any there adverse drug effects ? <mask>',\n",
       " 'Sotalol-induced bradycardia reversed by glucagon. Are any there adverse drug effects ? <mask>',\n",
       " 'The cases are important in documenting that drug-induced dystonias do occur in patients with dementia, that risperidone appears to have contributed to dystonia among elderly patients, and that the categorization of dystonic reactions needs further clarification. Are any there adverse drug effects ? <mask>',\n",
       " 'An encephalopathy and cardiomyopathy developed in a seventeen-year-old girl with chemotherapy-induced renal failure while receiving an intravesical aluminum infusion for hemorrhagic cystitis. Are any there adverse drug effects ? <mask>',\n",
       " 'We describe a patient who developed HUS after treatment with mitomycin C (total dose 144 mg/m2) due to a carcinoma of the ascending colon. Are any there adverse drug effects ? <mask>',\n",
       " 'The authors caution that treatment with alprazolam may be complicated by the induction of mania. Are any there adverse drug effects ? <mask>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PETDatasetForClassification(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        processed_text : List[str], \n",
    "        labels : List[int],\n",
    "        tokenizer : AutoTokenizer,\n",
    "        device : str = \"cuda\"\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokens = tokenizer(\n",
    "            processed_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "        self.encoded_labels = deepcopy(self.tokens['input_ids'])\n",
    "        \n",
    "        self.encoded_labels[self.encoded_labels != tokenizer.mask_token_id] = -100\n",
    "\n",
    "        for idx, sentence in enumerate(self.encoded_labels):\n",
    "            sentence[sentence == tokenizer.mask_token_id] = tokenizer.vocab[labels[idx].lower()]\n",
    "\n",
    "        self.inputs : Dict[str, torch.Tensor] = self.tokens\n",
    "        self.inputs['labels'] = self.encoded_labels\n",
    "\n",
    "        for k,v in self.inputs.items():\n",
    "            self.inputs[k] = v.to(device)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        d : Dict = dict()\n",
    "        for key in self.inputs.keys():\n",
    "            d[key] = self.inputs[key][index]\n",
    "\n",
    "        return d\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.tokens['input_ids'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PETDatasetForClassification(\n",
    "    train_text,\n",
    "    train_labels,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "val_dataset = PETDatasetForClassification(\n",
    "    val_text,\n",
    "    val_labels,\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, shuffle=True)\n",
    "val_dataloader =DataLoader(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import FocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([35, 15]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score : 0.0\n",
      "tensor([[14,  0],\n",
      "        [ 6,  0]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForMaskedLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFacebookAI/roberta-large\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      2\u001b[0m                                              cache_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Data/pedro.silva\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m history, confusion_matrix, best_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbalizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mverbalizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbinary_f1_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFocalLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/src/model/mlm/pet_for_text_classification.py:88\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_dataloader, val_dataloader, num_classes, model, verbalizer, tokenizer, alpha, evaluation_fn, loss_fn, device, lr, n_epochs)\u001b[0m\n\u001b[1;32m     83\u001b[0m loss_ce\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     85\u001b[0m loss \u001b[38;5;241m=\u001b[39m alpha\u001b[38;5;241m*\u001b[39mloss_mlm \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39malpha)\u001b[38;5;241m*\u001b[39m loss_ce\n\u001b[0;32m---> 88\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     90\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"FacebookAI/roberta-large\",\n",
    "                                             cache_dir = \"/Data/pedro.silva\").to(device)\n",
    "history, confusion_matrix, best_model = train(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    num_classes = 2,\n",
    "    model = model,\n",
    "    verbalizer = verbalizer,\n",
    "    tokenizer=tokenizer,\n",
    "    alpha = 1e-4,\n",
    "    evaluation_fn= binary_f1_score,\n",
    "    loss_fn=FocalLoss(class_weights, gamma=2),\n",
    "    n_epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
