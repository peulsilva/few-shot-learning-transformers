{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import LayoutLMForTokenClassification,\\\n",
    "    LayoutLMTokenizer, AdamW, LayoutLMv2Processor, LayoutLMv2ForTokenClassification\n",
    "# from tensordict import TensorDict\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import ImageDraw, Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import logging\n",
    "# from torchvision.transforms import PILToTensor\n",
    "from torcheval.metrics.functional import multiclass_f1_score, \\\n",
    "    multiclass_accuracy\n",
    "\n",
    "import os \n",
    "if 'notebooks' in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "    \n",
    "from src.preprocessing.make_dataset import ImageLayoutDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='logs/train.log', encoding='utf-8', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"katanaml/cord\", )\n",
    "\n",
    "# dataset = load_dataset(\"darentang/sroie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating PyTorch Datasets, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", revision=\"no_ocr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:38<00:00, 20.54it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df = ImageLayoutDataset(dataset['train'], encode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.34it/s]\n"
     ]
    }
   ],
   "source": [
    "val_df = ImageLayoutDataset(dataset['validation'], encode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_df,\n",
    "    shuffle=True,\n",
    "    batch_size= 2\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    val_df,\n",
    "    shuffle= True,\n",
    "    batch_size= 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_rows = []\n",
    "for row in dataset['train']['ner_tags']:\n",
    "    unique_rows.append(np.unique(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = np.unique(np.concatenate(unique_rows)).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlm-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LayoutLMForTokenClassification(\n",
       "  (layoutlm): LayoutLMModel(\n",
       "    (embeddings): LayoutLMEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (x_position_embeddings): Embedding(1024, 768)\n",
       "      (y_position_embeddings): Embedding(1024, 768)\n",
       "      (h_position_embeddings): Embedding(1024, 768)\n",
       "      (w_position_embeddings): Embedding(1024, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): LayoutLMEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): LayoutLMPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=23, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = LayoutLMForTokenClassification.from_pretrained(\n",
    "    'microsoft/layoutlm-base-uncased',\n",
    "    num_labels=n_labels\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of treinable / non-treinable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Model Info\n",
      "    -----------------\n",
      "    \n",
      "    Treinable params: 112645655\n",
      "    Non Treinable params: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'''\n",
    "    Model Info\n",
    "    -----------------\n",
    "    \n",
    "    Treinable params: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\n",
    "    Non Treinable params: {sum(p.numel() for p in model.parameters() if not p.requires_grad)}\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/pedro.silva/few-shot-learning-transformers/.venv/lib64/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 4/4 [01:37<00:00, 24.42s/it]\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "global_step = 0\n",
    "num_train_epochs = 4\n",
    "\n",
    "#put the model in training model\n",
    "logging.info('''\n",
    "   Starting model training\n",
    "   -----------------------       \n",
    "''')\n",
    "model.train()\n",
    "history = {\n",
    "   \"train_accuracy\": [],\n",
    "   \"train-f1\": [],\n",
    "   \"validation-accuracy\": [],\n",
    "   \"validation-f1\": []\n",
    "}\n",
    "\n",
    "for epoch in tqdm(range(num_train_epochs)):\n",
    "   logging.info(f\"Epoch: {epoch}\")\n",
    "   train_f1 = []\n",
    "   validation_f1 = []\n",
    "   train_accuracy = []\n",
    "   validation_accuracy = []\n",
    "   for X in (train_dataloader):\n",
    "      input_ids = X[\"input_ids\"]\\\n",
    "         .to(device)\\\n",
    "         .squeeze()\n",
    "      \n",
    "      bbox = X[\"bbox\"]\\\n",
    "         .to(device)\\\n",
    "         .squeeze()\n",
    "\n",
    "      attention_mask = X[\"attention_mask\"]\\\n",
    "         .to(device)\\\n",
    "         .squeeze()\n",
    "      token_type_ids = X[\"token_type_ids\"]\\\n",
    "         .to(device)\\\n",
    "         .squeeze()\n",
    "      \n",
    "      labels = X[\"labels\"]\\\n",
    "         .to(device)\\\n",
    "         .squeeze()\n",
    "\n",
    "      # image = X[\"image\"]\\\n",
    "      #    .to(device)\\\n",
    "      #    .squeeze()\n",
    "\n",
    "      # forward pass\n",
    "      outputs = model(\n",
    "         input_ids=input_ids, \n",
    "         bbox=bbox, \n",
    "         attention_mask=attention_mask, \n",
    "         token_type_ids=token_type_ids,\n",
    "         labels=labels\n",
    "      )\n",
    "      \n",
    "      loss = outputs.loss\n",
    "      predictions = outputs.logits.argmax(-1)\n",
    "\n",
    "      valid_outputs_mask = labels != -100\n",
    "\n",
    "      # backward pass to get the gradients \n",
    "      loss.backward()\n",
    "\n",
    "      # update\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "      global_step += 1\n",
    "      \n",
    "      acc = multiclass_accuracy(\n",
    "         predictions[valid_outputs_mask],\n",
    "         labels[valid_outputs_mask],\n",
    "         num_classes=n_labels\n",
    "      )\n",
    "\n",
    "      f1 = multiclass_f1_score(\n",
    "         predictions[valid_outputs_mask],\n",
    "         labels[valid_outputs_mask],\n",
    "         num_classes=n_labels\n",
    "      )\n",
    "\n",
    "      train_accuracy.append(acc)\n",
    "      train_f1.append(f1)\n",
    "\n",
    "   with torch.no_grad():\n",
    "      for X_validation in validation_dataloader:\n",
    "         input_ids = X_validation[\"input_ids\"]\\\n",
    "         .to(device)\\\n",
    "         .squeeze()\n",
    "      \n",
    "      bbox = X_validation[\"bbox\"]\\\n",
    "         .to(device)\\\n",
    "         .squeeze()\n",
    "\n",
    "      attention_mask = X_validation[\"attention_mask\"]\\\n",
    "         .to(device)\\\n",
    "         .squeeze()\n",
    "      token_type_ids = X_validation[\"token_type_ids\"]\\\n",
    "         .to(device)\\\n",
    "         .squeeze()\n",
    "      \n",
    "      labels = X_validation[\"labels\"]\\\n",
    "         .to(device)\\\n",
    "         .squeeze()\n",
    "\n",
    "      # image = X_validation[\"image\"]\\\n",
    "      #    .to(device)\\\n",
    "      #    .squeeze()\n",
    "\n",
    "      # forward pass\n",
    "      outputs = model(\n",
    "         input_ids=input_ids, \n",
    "         bbox=bbox, \n",
    "         attention_mask=attention_mask, \n",
    "         token_type_ids=token_type_ids,\n",
    "         labels=labels\n",
    "      )\n",
    "\n",
    "      loss = outputs.loss\n",
    "      predictions = outputs.logits.argmax(-1)\n",
    "\n",
    "      valid_outputs_mask = labels != -100\n",
    "\n",
    "      correct = (predictions == labels)\\\n",
    "         [valid_outputs_mask]\\\n",
    "         .float()\\\n",
    "         .sum()\n",
    "      \n",
    "      acc = multiclass_accuracy(\n",
    "         predictions[valid_outputs_mask],\n",
    "         labels[valid_outputs_mask],\n",
    "         num_classes=n_labels\n",
    "      )\n",
    "\n",
    "      f1 = multiclass_f1_score(\n",
    "         predictions[valid_outputs_mask],\n",
    "         labels[valid_outputs_mask],\n",
    "         num_classes=n_labels\n",
    "      )\n",
    "\n",
    "      validation_accuracy.append(acc)\n",
    "      validation_f1.append(f1)\n",
    "\n",
    "   avg_train_acc = torch.tensor(train_accuracy)\\\n",
    "      .mean()\\\n",
    "      .item()\n",
    "   \n",
    "   avg_train_f1 = torch.tensor(train_f1)\\\n",
    "      .mean()\\\n",
    "      .item()\n",
    "   \n",
    "   avg_val_acc = torch.tensor(validation_accuracy)\\\n",
    "      .mean()\\\n",
    "      .item()\n",
    "   \n",
    "   avg_val_f1 = torch.tensor(validation_f1)\\\n",
    "      .mean()\\\n",
    "      .item()\n",
    "\n",
    "\n",
    "   history[\"train_accuracy\"].append(avg_train_acc)\n",
    "   history['train-f1'].append(avg_train_f1)\n",
    "   history['validation-accuracy'].append(avg_val_acc)\n",
    "   history['validation-f1'].append(avg_val_f1)\n",
    "\n",
    "   logging.info(\n",
    "      f'''\n",
    "      End of epoch {epoch}\n",
    "      ---------------------\n",
    "      Training accuracy: {avg_train_acc}\n",
    "      Tranining f1-score : {avg_train_f1}\n",
    "      Validation accuracy: {avg_val_acc}\n",
    "      Validation f1-score : {avg_val_f1}\n",
    "      ''', \n",
    "   )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
